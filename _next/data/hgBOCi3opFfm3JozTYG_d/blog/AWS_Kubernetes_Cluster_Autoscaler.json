{"pageProps":{"post":{"title":"How to (Cluster) Autoscale a self hosted Kubernetes on AWS EC2","date":"2024-04-29","slug":"AWS_Kubernetes_Cluster_Autoscaler","author":"Per Sunde","content":"\n# Autoscale K3S on AWS\n\n## TLDR\n\n1. Setup the IAM configuration\n   1. Create an IAM Security policy\n   2. Create an IAM Role\n      1. Attach the newly created policy to this role\n2. Setup the Control plane\n   1. Create one or more EC2's\n      1. Set the newly created IAM Role on the EC2\n   2. Install K3S\n      1. Make sure to include `INSTALL_K3S_EXEC=\"--kubelet-arg=provider-id=aws:///$(ec2metadata --availability-zone)/$(ec2metadata --instance-id)\"` in the installation script\n3. Create an EC2 Auto Scaling Group (ASG)\n   1. Add \"tags\" on the ASG.\n      1. Needed so that the cluster-autoscaler identifies and uses the correct ASG.\n   2. Add the \"user data\" script.\n      1. Will install K3S.\n      2. Have the EC2 automatically join the cluster as an \"Agent\".\n4. Deploy the cluster-autoscaler\n   1. Configure it to use the ASG by looking for ASG's with the specified tags: `--node-group-auto-discovery=asg:tag=my-autoscaler-tag1,my-autoscaler-tag2`\n5. Deploy a test application to test the Cluster Autoscaler\n\n## 1. Setup the IAM configuration\n\nCreate the [Full Cluster Autoscaler Features Policy (Recommended)](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#full-cluster-autoscaler-features-policy-recommended)\nBut remove the \"eks:DescribeNodegroup\" action, since we are not using EKS.\n\n>NOTE: This gives all the permissions and allow it for all resources. If you want to be more restrictice, check out the [Minimal IAM Permissions Policy](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#minimal-iam-permissions-policy) to see how you can restrict which resources can be granted these permissions.\n\nGo to IAM -> Policies -> \"Create Policy\" -> JSON and copy paste this text into the policy editor and give it a name.\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"autoscaling:DescribeAutoScalingGroups\",\n        \"autoscaling:DescribeAutoScalingInstances\",\n        \"autoscaling:DescribeLaunchConfigurations\",\n        \"autoscaling:DescribeScalingActivities\",\n        \"autoscaling:DescribeTags\",\n        \"ec2:DescribeImages\",\n        \"ec2:DescribeInstanceTypes\",\n        \"ec2:DescribeLaunchTemplateVersions\",\n        \"ec2:GetInstanceTypesFromInstanceRequirements\"\n      ],\n      \"Resource\": [\"*\"]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"autoscaling:SetDesiredCapacity\",\n        \"autoscaling:TerminateInstanceInAutoScalingGroup\"\n      ],\n      \"Resource\": [\"*\"]\n    }\n  ]\n}\n```\n\nNext create a IAM Role, give it a descriptive name, and attach this newly created policy. Or add this policy to your existing IAM Role that you use.\n\n## 2. Setup the Control plane\n\nCreate one (or more) EC2 instance(s) that will work as your controle-plane nodes. These will not be a part of the auto-scaling.\nThese EC2 instances should have the \"IAM Role\" you created in the previous step.\n\nRead the up to date documentation about how to setup and install K3S here: <https://k3s.rocks/install-setup/>\nFollow the guide, but when you come to the \"Install k3s\" section, instead run the command below instead.\nSince we are not using EKS, we need to specify the kubelet \"provider-id\".\n\n\n**Install system dependencies:**\n\n```sh\nsudo apt update && \\\nsudo apt upgrade -y && \\\nsudo apt install -y curl  \\\n  ca-certificates \\\n  open-iscsi \\\n  wireguard \\\n  nfs-common\n```\n\n**Install K3S:**\n\n>**NOTE:** Assumes you use AWS Ubuntu. If you use AWS Linux or non-debian based system, then the command `ec2metadata` might have a different name. For AWS Linux the command is `ec2-metadata`.\n\nOnly difference between the installation command in <https://k3s.rocks> and the below command is that you need to add `INSTALL_K3S_EXEC=\"--kubelet-arg=provider-id=aws:///$(ec2metadata --availability-zone)/$(ec2metadata --instance-id)\"` before `sh` to the install command!\n\n>\"If you're managing your own kubelets, they need to be started with the --provider-id flag. The provider id has the format aws:///\\<availability-zone>/\\<instance-id>, e.g. aws:///us-east-1a/i-01234abcdef.\"\n>[Source](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#common-notes-and-gotchas)\n\nSet the \"K3S_VERSION\" variable to the version you want to install. See <https://k3s.rocks> for the latest version.\nOptionally remove the `INSTALL_K3S_VERSION=$K3S_VERSION` part to install the latest stable version.\n\n```sh\nK3S_VERSION=v1.28.3+k3s2\n\ncurl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=$K3S_VERSION INSTALL_K3S_EXEC=\"--kubelet-arg=provider-id=aws:///$(ec2metadata --availability-zone)/$(ec2metadata --instance-id)\" sh -s server \\\n--cluster-init \\\n--flannel-backend=wireguard-native \\\n--write-kubeconfig-mode 644 && \\\nexport KUBECONFIG=/etc/rancher/k3s/k3s.yaml && \\\ncat traefik-config.yaml | envsubst | kubectl apply -f -\n```\n\n## 3. Create an EC2 Auto Scaling Group (ASG)\n\n> >Note: **Cluster Autoscaler is not responsible for behaviour and registration to the cluster of the new nodes it creates.** The responsibility of registering the new nodes into your cluster lies with the cluster provisioning tooling you use. Example: If you use kubeadm to provision your cluster, it is up to you to automatically execute kubeadm join at boot time via some script.\n>\n>[Source](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-up-work)\n>\n>It is up to you to make sure the newly created EC2 instance joins the Kubernetes cluster. That is why we add an init script to the ASG, to make sure newly added EC2 instancess install K3S and join the cluster automatically.\n\nInit script for Auto-Scale Group (AGS).\nThis will join a new agent node to the cluster.\n\nAdd the K3S_TOKEN and the MASTER_IP to the script.\n\n- K3S_TOKEN: Run this command on one of your control-plane nodes: `k3s token create --ttl 0`\n  - It will generate a token that will never expire, so make sure to not leak the secret!\n- MASTER_IP: The private IP of your controle-plane instance. Found on the EC2 dashboard.\n\n```sh\n#!/bin/bash\n#NOTE: Assumes you use AWS Ubuntu\n\n# Install system dependencies\napt update && \\\napt upgrade -y && \\\napt install -y curl  \\\nca-certificates \\\nopen-iscsi \\\nwireguard \\\nnfs-common\n\nK3S_TOKEN=\"<JOIN_TOKEN>\"\nMASTER_IP=\"<CONTROL_PLANE_PRIVATE_IP\"\nAVAILABILITY_ZONE=$(ec2metadata --availability-zone)\nINSTANCE_ID=$(ec2metadata --instance-id)\n\n# Join Agent node\ncurl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.28.3+k3s2 INSTALL_K3S_EXEC=\"--kubelet-arg=provider-id=aws:///$AVAILABILITY_ZONE/$INSTANCE_ID\" K3S_TOKEN=\"${K3S_TOKEN}\" K3S_URL=https://${MASTER_IP}:6443 sh -\n```\n\n## 4. Deploy the cluster-autoscaler\n\nClone the cluster-autoscaler repo:\n\n```sh\ngit clone https://github.com/kubernetes/autoscaler.git\ncd autoscaler/cluster-autoscaler/cloudprovider/aws/\n```\n\nModify the example deployment file.\n\n1. Change the node-group-auto-discovery tags.\n2. (If you do not use AWS Linux) Change the ssl-certs path\n   1. See [Common Notes and Gotchas](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md#common-notes-and-gotchas)\n3. Deploy the Cluster Autoscaler\n\n**Step 1 - Change the node-group-auto-discovery tags**\nUnder the \"Deployment\" kind, modify the `\"--node-group-auto-discovery=asg:tag=...\"` part. Here you need to use the tags you set on your ASG. The ASG has to contain all the tags specified, but the ASG can contain additional tags, but it can not be missing any tags.\n\nExample:\n\n```yaml\n--node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/control-plane1\n```\n\nThis one will match for an ASG that has the two tags \"k8s.io/cluster-autoscaler/enabled\" and \"k8s.io/cluster-autoscaler/control-plane1\".\n\n**Step 2 - Change the ssl-certs path**\n>NOTE: If you use Ubuntu or something other than AWS Linux, you need to modify this part.\n\nIn the file `examples/cluster-autoscaler-autodiscover.yaml` under the cluster-autoscaler deployment, change the ssl-certs volume.\n\n```yaml\n## From this\n      volumes:\n        - name: ssl-certs\n          hostPath:\n            path: \"/etc/ssl/certs/ca-bundle.crt\"\n\n## To this\n      volumes:\n        - name: ssl-certs\n          hostPath:\n            path: \"/etc/ssl/certs/ca-certificates.crt\"\n```\n\n**Step 3 - Deploy the Cluster Autoscaler**\nApply the autoscaler deployment:\n\n```sh\nkubectl apply -f examples/cluster-autoscaler-autodiscover.yaml\n```\n\nNow the auto-scaler should be deployed. Check that it works by running\n\n```sh\nkubectl get pods -A\n# or\nkubectl get pods -n=kube-system\n```\n\nGet the pod ID, and check the logs\n\n```sh\nkubectl logs cluster-autoscaler-<ID> -n=kube-system\n```\n\nOr check the description and see if there are any issues with deploying the pod:\n\n```sh\nkubectl describe pod cluster-autoscaler-<ID> --namespace=kube-system\n```\n\n## 5. Test the autoscaling\n\nDeploy a test deployment that you can use to see if the autoscaler works or not:\n\nSave the file as `hello-world.yaml` and deploy it `kubectl apply -f hello-world.yaml`\n\n```yaml hello-world.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hello-world\n  name: hello-world\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hello-world\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: hello-world\n    spec:\n      containers:\n      - name: ubuntu\n        image: ubuntu:latest\n        # Prints \"HELLO WORLD\" every 30 seconds\n        command: [ \"/bin/bash\", \"-c\", \"--\" ]\n        args: [ \"while true; do echo 'HELLO WORLD' && sleep 30; done;\" ]\n        resources:\n          limits:\n            cpu: 500m\n            memory: 512Mi\n          requests:\n            cpu: 500m\n            memory: 512Mi\n```\n\nThe autoscaler will check every 10 second if it needs to scale up or down or do nothing.\nCheck the ASG on the AWS EC2 dashboard, and see if the desired count is changed or not.\n\n>**NOTE:** Make sure your \"Maximum capacity\" in the ASG is set above 0 so that it can scale up. The Cluster Autoscaler will not scale over or under the maximum or minimum number you set in the ASG.\n\nChange the number of replicas in the `hello-world.yaml` and apply the file again to see the autoscaler start to scale up or down.\nNote that the default time to scale down is 10 minutes after node has been marked for removal. After 10 minutes, if the node is still ok to be removed the Cluster Autoscaler will terminate the EC2 instance.\n\nRead the [AWS Autoscaler README](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md) for how you can change the parameters for the cluster autoscaler that was deployed using the `examples/cluster-autoscaler-autodiscover.yaml` file.\n\n## NOTES\n\n- The Cluster Autoscaler \"does not delete the Node object from Kubernetes.\"\n  >How does Cluster Autoscaler remove nodes?\n  >\n  >Cluster Autoscaler terminates the underlying instance in a cloud-provider-dependent manner.\n  >\n  >It does not delete the Node object from Kubernetes. Cleaning up Node objects corresponding to terminated instances is the responsibility of the cloud node controller, which can run as part of kube-controller-manager or cloud-controller-manager.\n  >\n  >[Source](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-cluster-autoscaler-remove-nodes)\n\n- If you want to delete the \"Node object\" you can either use your own bash script or similar, or run the [AWS cloud provider](https://cloud-provider-aws.sigs.k8s.io/)\nOr make your own Operator that automatically does this for you.\n  - Example bash script: `kubectl delete node $(kubectl get nodes | grep NotReady | awk '{print $1;}')` \n    - [Source](https://stackoverflow.com/a/68001385/15137927)\n\nThere are lots of details you should be aware of in a production setting. Be sure to read the Cluster Autoscaler [FAQ](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md) and the [AWS README](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md).\n\n","featuredImage":"/assets/blog/ec2+k8s/ec2+k8s.png","readingTime":"8 min read","tags":"Kubernets, K8S, K3S, AWS, EC2, hosting, autoscale"}},"__N_SSG":true}